4/26

Algorithms compatible with log loss takes in a probability (0,1) value
  naive bayes (likelyhood)
  logistic regeression (logit)
  entropy based tree models (odds)
 
    Note:
    SVM is not in this category. Margin can somewhat show the confidence of the classification, but not of same scale as probability.
 
Feature analysis:
 
 (longitude, latitude) -> location for clustering -> 0-1 one hot encoding
        assuming there's a difference caused by location
        hyperparameter: numbers of clusters
        
 street addresss/building ID -> further identifies the exact building
        solves the problem of near neighborhood different tier quality
        prone to overfitting, very sparse
 
 (bathroom, bedroom) -> choice of apartment -> 0-1 categorical
        assuming this is fixed demand 
        while more bb will increase in price, the increase is linear?
 
 features -> 0-1 categorical features
        
 descriptions -> unstructured text -> hand engineered 0-1 categories (TF-IDF may not be useful)
        find new features by word count using stop words and stemmers
        Note:        features extracted in text may have common part in "features". Should we take union or use separately?
                     This depends on our assumption of the effectiveness of communication (is "features" as powerful as "description")

 manager_id -> 0-1 one hot
        some good managers can make the same listing more appealing?
        another very sparse feature

 photos -> 1-0 categorical: 1(num_photo > x) or something like {0,1-2,3-4,5+} categorical
         seeing the plot of nums of photos has little correlation with the satisfiction maybe we just need a threshold
         well shot photoes should show corresponding price of the listing but in fact highly noisy
 
 created_time -> ymdh
            1.price adjustment
            price of listings may have a periodical fluctuation along time
            price of listings may in/decrease along time
            2.information exposure
            posting a listing at some time of the day may have more exposure
            Note:   prone to overfitting better finding a distribution
 
 price -> only continuous variable here
        logprice? the rich are less sensitive to high price?
 
 Summary of feature analysis:
      lots of features (num_cluster+num_features_with_descrip is alreadly a lot) with sparse 0-1 encoding
      highly prone to overfitting
      
 Solutions:
 
  Models applicable:
    boosting + bagging/CV to overcome overfitting
    XGboost (should have best performance)
    Random Forest (ensemble of decision trees)
    Boosted logistic regression/naive bayes (FSAM with log loss) better than single model
    
    The paper shows that XGboost with log loss is best
    http://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf
    
    
 Special thoughts on Price
      it's possible to regress price ~ other_features
      can use pricehat as an extra feature for classification
      why or why not? Need to be tested.
       
