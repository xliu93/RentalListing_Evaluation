4/26
5/2*

Algorithms compatible with log loss takes in a probability (0,1) value
  naive bayes (likelyhood)
  logistic regeression (logit)
  entropy based tree models (odds)
 
    Note:
    SVM is not in this category. Margin can somewhat show the confidence of the classification, but not of same scale as probability.
    Can use log loss for the yhat, but not optimize on log loss.
    problem: the probability returned by most classifiers optimized on log-loss is biased
   *Solution: 1. using log loss based models (logistic regression)
              2. using probability calibration (sklearn.calibration) not seen on Kaggle!
 
Feature analysis:
 
 (longitude, latitude) -> location for clustering -> 0-1 one hot encoding
        assuming there's a difference caused by location
        hyperparameter: numbers of clusters
        *data pattern: most data around NYC -74.05<longtitude<-73.75
                                            40.4<latitude<40.9
        method: In/OUT NYC
        package: sklearn.cluster.birch
        note: 50k training 75k testing
        
        
 street addresss/building ID -> further identifies the exact building
        solves the problem of near neighborhood different tier quality
        prone to overfitting, very sparse
 
 (bathroom, bedroom) -> choice of apartment -> 0-1 categorical
        assuming this is fixed demand 
        while more bb will increase in price, the increase is linear?
 
 features -> 0-1 categorical features
        
 descriptions -> unstructured text -> hand engineered 0-1 categories (TF-IDF may not be useful)
        find new features by word count using stop words and stemmers
        Note:        features extracted in text may have common part in "features". Should we take union or use separately?
                     This depends on our assumption of the effectiveness of communication (is "features" as powerful as "description")
        
 manager_id -> 0-1 one hot
        some good managers can make the same listing more appealing?
        another very sparse feature

 photos -> 1-0 categorical: 1(num_photo > x) or something like {0,1-2,3-4,5+} categorical (can be decided by tree?)
         seeing the plot of nums of photos has little correlation with the satisfiction maybe we just need a threshold
         well shot photoes should show corresponding price of the listing but in fact highly noisy
 
 created_time -> ymdh
            1.price adjustment
            price of listings may have a periodical fluctuation along time
            price of listings may in/decrease along time
            2.information exposure
            posting a listing at some time of the day may have more exposure
            Note:   prone to overfitting better finding a distribution
 
 price -> only continuous variable here
        logprice? the rich are less sensitive to high price?
 
 Summary of feature analysis:
      lots of features (num_cluster+num_features_with_descrip is alreadly a lot) with sparse 0-1 encoding
      highly prone to overfitting
      vc dimention?
      need regularization
      
 Solutions:
 
  Models applicable:
    boosting + bagging/CV to overcome overfitting
    XGboost (should have best performance)
              *Can compare with DART with dropout
    Random Forest (ensemble of decision trees)
    Boosted logistic regression (equivalent to Adaboost! which does have predict_proba(X)!) better than single model
              https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf
    *+probability calibration (Isotonic/Sigmoid)
              https://jmetzen.github.io/2015-04-14/calibration.html
     *boosted tree with Sigmoid calibration should benefit from probability calibration
              http://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf
    The paper shows that XGboost with log loss is best
              http://www.cs.cornell.edu/~caruana/niculescu.scldbst.crc.rev4.pdf
    
    
 Special thoughts on Price
      it's possible to regress price ~ other_features
      *can use (price-pricehat)/pricehat as an extra feature for classification
      why or why not? Need to be tested.
      *Someone added white noise in their kernel.Is the regression another kind of white noise or regularization?
      *How should we ensemble probabilities? geometric mean or arithmetic mean?(when DropOut/CV/additive to be applied)
